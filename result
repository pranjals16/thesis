IMPORTant
average of shuffled and not shuffled sentence vectors
92.14 logistic c=3.3; without rnnlm 
92.22 linear SVM c=0.29 without rnnlm 
92.71 c=4.5 rnnlm
-----------------------------------------
random forest: 280 trees
84.14
svm: linear (300 context;min count=20)
87.05
Naive Bayes
75.95
merged with Wiki trained(300 dim wiki trained context5)-svm
88.60
Logistic Regression with l1 penalty
86.90
k-nn:20
76.76
---------------------------------------------------------------------
Hindi

78.0% 50-fold cross validation (350+350 product review;context = 5 downsampling = 1e-5 word_count=1)
79.62% 20-fold cross validation (iitb movie review;context = 5 downsampling = 1e-5 word_count=1,feature=300) better than paper : a fall-back strategy


Merged word vectors with tf-idf
------------------
89.52% 20-fold cross validation (iitb movie review;context = 6 downsampling = 1e-3 word_count=1,feature=500) better than paper : a fall-back strategy
90.86% 20-fold cross validation (350+350 product review;context = 6 downsampling = 1e-3 word_count=1)


with stop words removed(vectorizer = TfidfVectorizer(tokenizer=tokenize,use_idf=True,max_df=0.3,min_df=0.0001,strip_accents='unicode'))
------------------------
89.97% 20-fold cross validation (iitb movie review;context = 6 downsampling = 1e-3 word_count=1,feature=500) better than paper : a fall-back strategy
91.14% 20-fold cross validation (350+350 product review;context = 6 downsampling = 1e-3 word_count=1)

-----

tf-idf+BOW results
------------------------------------------------------------
with pos+neg together idf includes tf-idf features
89.71 : IIIT
85.90 : IITB
92.89:IIIT:600,6,1e-4;svm(C=0.9)40-fold
90.30:IITB:500,7,1e-4;svm(C=0.8)40-fold

with separate idf_scores includes tf-idf features
93.71: IIIT
90.13: IITB 600 features


wihtout tf-idf features: pure weighted pos+neg separate average
92.86: IIIT
88.17: IITB  600 features

Doc2Vec
IITB:89.17;800,8,1e-4;svm(C=0.9)20-fold with feature engineering
IITB:79.17;800,8,1e-4;svm(C=0.6)20-fold
IIIT:86.86;800,7,1e-5;svm(C=0.8)20-fold
IIIT: 90.86;800,7,1e-5;svm(C=0.8)20-fold with feature engineering
---------------------------------------------------------------

parsed amazon dataset

Weighted Average Word Vectors
88.98: MP3  500 features context=10 (NE=10)
89.41: Electronics  500 features context=10
91.15: Watches 500 features context=10

Trip Advisor Dataset
91.57% accuracy: 600 context=10

--------------------------------------------------------------------------------
Movie Review Dataset(Mine)
Doc2Vec
------
74.57
88.07: with feature engineering

Without Feature Engineering:
75.86:500,5
76.43:600,5
77.44:600,6(c=0.9)

With Feature Engineering:
89.51:500,5
90.37: 600 select_k_best
78.61: PCA 50
66.67: LDA

92.53 with grid search select_k_best

--------------------------------------------------------------------------------
IMDB DocVec

1) PvDm: 72.52
2) PvDBOW: 74.77
3) 92.93 with logistic
4) 92.88 svm
5) 93.07 grid search
6) 93.33 rbf kernel svm

Word2vec+tfidf: 89.03%

--------------------------------------------------------
Word2vec: 88.42(pure average with varying C in SVM Linear)
Word2vec: 88.41 (weighted average with varying C in SVM Linear)
------------------------------------------------------------

docvec+word2vec
1) 93.09 logistic 100features_10minwords_10context
2) 92.98 svm 100features_10minwords_10context
3) 93.24 svm 200features_10minwords_10context linear

docvec+word2vec+wiki
1) 93.09 logistic wiki context old 200features_10minwords_10context
2) 93.18 logistic wiki context 10 200features_10minwords_10context C=1.6
3) 93.03 logistic wiki context 5 200features_10minwords_10context
4) 93.18 with weighted logistic wiki context 10 200features_10minwords_10context C=1.6
------------------
wordvec+docvec+rnnlm
93.468 with RNNLM+docvec
93.70  (c=6.5,100features+100) 50 nodes in RNNLM

93.57: word+doc+wiki+rnnlm hidden=50
93.93 word+doc+tfidf+rnnlm c=2.1 hidden=50
94.19 word+doc+tfidf+rnnlm c=8.5 without stop words hidden=50

word+doc+tfidf
93.448 ngram=1,2;features=25000
93.456 ngram=1,2;features=20000
93.64 ngram=1,2;features=15000 (C=0.2 LinearSVC)
93.91 ngram=1,2;features=15000 stop_words=false svm C=0.33
93.83 ngram=1,2;features=15000 stop_words=false logistic l2 c=4.3

word+doc+wiki+tfidf
93.55 linear svm c=0.3

weighted word: 88.42
weighted word+ doc: 93.19 linear svm
------------------
--------------------------------------------------------------------------------
To do in thesis
---------------
1) Graph of run on different datasets
2) Wikipedia different context size tsne plot
3) k-means plot
4) PCA new chapter with comparison in Hindi
5) Recurrent Network Different Chapter
6) Concatenation of vector space Topic
7) Chapter on Embeddings

1st: weighted average alone with logistic l1,l and svm linear,rbf
