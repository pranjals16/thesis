\cleardoublepage

\begin{center}
	\huge{\textbf{Abstract}}
\end{center}

In recent years, distributional semantics or word vector models have been proposed to capture both the syntactic and semantic similarity between words.  Since these can be obtained in an unsupervised manner, they are of interest for under-resourced languages such as Hindi.  We test the efficacy of such an approach for Hindi, first by a subjective overview which shows that a reasonable measure of word similarity seems to be captured quite easily.  We then apply it to the sentiment analysis for two small Hindi databases from earlier work. 

In order to handle larger strings from the word vectors, several methods - additive, multiplicative, or tensor neural models, have been proposed.  Here we find that even the simplest - an additive average, results in an impressive accuracy gain on state of the art by 10\% (from 80\%) for two review datasets.  The results suggest that it may be worthwhile to explore such methods further for Indian languages.
